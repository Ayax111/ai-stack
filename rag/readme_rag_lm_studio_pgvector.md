# RAG + LM Studio + Postgres/pgvector â€” README

Este proyecto implementa un **pipeline RAG local**: ingesta de documentos â†’ embeddings â†’ almacenamiento en Postgres con **pgvector** â†’ recuperaciÃ³n semÃ¡ntica â†’ generaciÃ³n de respuestas con un **LLM servido por LM Studio**. Todo corre en tu VM de Ubuntu (cliente) y un host con LM Studio en LAN.

---

## 1) Arquitectura (visiÃ³n general)

```
[data/*.txt|.md|.pdf]
        â”‚
        â–¼
  ingest.py     â†’ troceado (chunking) + embeddings (LM Studio o ST)
        â”‚
        â–¼
Postgres + pgvector  â†’ tabla `documents(â€¦ embedding VECTOR(d))`
        â–²
        â”‚   query.py â†’ embebe la consulta, recupera topâ€‘k por similitud (cosine)
        â”‚
        â””â”€â”€ llm_client.py â†’ LM Studio (/v1/chat/completions) para redactar la respuesta
```

- **Ingesta:** trocea tus documentos y calcula **embeddings** de cada fragmento; los guarda en Postgres con Ã­ndice vectorial (**HNSW** + cosine).
- **Consulta:** convierte tu pregunta en embedding, recupera los **k** pasajes mÃ¡s similares y arma un **prompt con contexto** para que el LLM responda **anclado en tus fuentes**.
- **LM Studio:** servidor **OpenAIâ€‘compatible** en tu LAN (p. ej., `http://192.168.0.194:1234/v1`).

---

## 2) Requisitos

- Ubuntu 22.04/24.04 con Python 3.11 (venv), Node opcional, Docker.
- Contenedor Postgres con extensiÃ³n **pgvector**.
- **LM Studio** ejecutÃ¡ndose en el host (modo Developer â†’ Local Server).

---

## 3) Estructura del proyecto

```
ai-stack/
â”œâ”€ rag/
â”‚  â”œâ”€ .venv/                 # entorno virtual de Python
â”‚  â”œâ”€ ingest.py              # ingesta: troceo + embeddings + upsert
â”‚  â”œâ”€ query.py               # consulta: retrieve + prompt + LLM
â”‚  â”œâ”€ llm_client.py          # cliente de chat LM Studio (OpenAIâ€‘compatible)
â”‚  â”œâ”€ .env                   # configuraciÃ³n central del proyecto
â”‚  â””â”€ data/                  # tus documentos (puede cambiarse con DATA_DIR)
â”œâ”€ infra/
â”‚  â””â”€ docker-compose.yml     # Postgres + pgvector
â””â”€ mcp/, agent/              # (reservados para siguientes fases)
```

---

## 4) ConfiguraciÃ³n (.env)

El fichero `rag/.env` es **el panel de mandos**. Variables clave:

```dotenv
# â€” LLM de chat (LM Studio) â€”
LLM_BASE_URL=http://192.168.0.194:1234/v1
LLM_API_KEY=lm-studio                 # cadena cualquiera
LLM_MODEL=llama-3.2-3b-instruct-uncensored  # elige un modelo de chat de /v1/models

# â€” Base de datos â€”
DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:5432/ragdb

# â€” Documentos â€”
DATA_DIR=/home/ayax/ai-stack/rag/data     # carpeta con .txt .md .pdf (recursivo)
CHUNK_SIZE=800
CHUNK_OVERLAP=120

# â€” Embeddings â€” (opciÃ³n A: LM Studio)
EMBEDDING_BACKEND=lmstudio
EMBEDDING_BASE_URL=http://192.168.0.194:1234/v1
EMBEDDING_API_KEY=lm-studio
EMBEDDING_MODEL=text-embedding-embeddinggemma-300m
# EMBEDDING_ENDPOINT=embeddings          # opcional; fallback automÃ¡tico a 'embedding'
# EMBEDDING_DIM=768                      # opcional para evitar probe inicial

# â€” Embeddings â€” (opciÃ³n B: locales con Sentence-Transformers)
# EMBEDDING_BACKEND=sentence-transformers
# EMBEDDING_MODEL=BAAI/bge-m3
```

> **Consejo:** comprueba los modelos cargados con:
> `curl http://192.168.0.194:1234/v1/models`
> y pon en `LLM_MODEL`/`EMBEDDING_MODEL` el **id exacto** que te devuelva.

---

## 5) Puesta en marcha

### 5.1 Base de datos

```bash
cd ~/ai-stack/infra
docker compose up -d
```

### 5.2 LM Studio

1. Abre **Developer â†’ Local Server**.
2. Selecciona un **modelo de chat** y/o un **modelo de embeddings** (pueden convivir).
3. Puerto: `1234` Â· Bind: `0.0.0.0` Â· **Run**.
4. Verifica desde la VM:
   ```bash
   curl http://192.168.0.194:1234/v1/models
   curl -s http://192.168.0.194:1234/v1/embeddings \
     -H "Authorization: Bearer lm-studio" -H "Content-Type: application/json" \
     -d '{"model":"text-embedding-embeddinggemma-300m","input":"hola"}' | jq '.data[0].embedding | length'
   ```

### 5.3 Dependencias Python (solo una vez)

```bash
cd ~/ai-stack/rag
python -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install openai httpx python-dotenv sqlalchemy "psycopg[binary]" pgvector \
            langchain pypdf sentence-transformers numpy
```

### 5.4 Ingesta de documentos

Pon tus ficheros en `DATA_DIR` (por defecto `rag/data/`) y ejecuta:

```bash
cd ~/ai-stack/rag && source .venv/bin/activate
python ingest.py
```

Salida esperada:
```
ðŸ“‚ Usando DATA_DIR: /home/ayax/ai-stack/rag/data
ðŸ”Ž Embeddings LM Studio â†’ http://192.168.0.194:1234/v1/embeddings | model=text-embedding-embeddinggemma-300m
âœ… Ingesta completada.
```

### 5.5 Consulta

```bash
python query.py "Â¿QuÃ© es MCP?"
```

La respuesta citarÃ¡ fragmentos en el formato `[doc#chunk]`.

---

## 6) Â¿CÃ³mo funciona por dentro?

### 6.1 `ingest.py`
- **Lee** variables de `.env` (DB, DATA_DIR, chunking, backend de embeddings).
- **Trocea** cada documento con `RecursiveCharacterTextSplitter(CHUNK_SIZE, CHUNK_OVERLAP)`.
- **Embeddings** (a elecciÃ³n):
  - **LM Studio**: llamada HTTP directa (con `httpx`) a `/v1/embeddings` con `EMBEDDING_MODEL`.
  - **Sentence-Transformers**: modelo local (normaliza para cosine).
- **Schema**: crea `documents` con `embedding VECTOR(d)` e Ã­ndice `HNSW (vector_cosine_ops)` si no existen.
- **Inserta** filas (`doc_id`, `chunk_id`, `content`, `metadata`, `embedding`).

### 6.2 `query.py`
- **Embebe** la pregunta con **el mismo backend** de embeddings.
- **Consulta** por similitud:
  ```sql
  SELECT ..., 1 - (embedding <=> :qvec) AS score
  FROM documents
  ORDER BY embedding <=> :qvec
  LIMIT :k;
  ```
- **Contexto**: concatena los `k` pasajes mÃ¡s similares.
- **GeneraciÃ³n**: envÃ­a `system + user` a `llm_client.py`, que usa LM Studio `/v1/chat/completions`.

### 6.3 `llm_client.py`
- Cliente **OpenAIâ€‘compatible** apuntando a `LLM_BASE_URL`.
- ParÃ¡metros Ãºtiles: `temperature`, mensajes de `system` y `user`.

---

## 7) Cambiar de modelo de embeddings (dimensiÃ³n)

pgvector exige una **dimensiÃ³n fija** por columna. Si cambias `EMBEDDING_MODEL` y la dimensiÃ³n no coincide con la ya almacenada:

```bash
docker exec -it infra-pgvector-1 psql -U postgres -d ragdb -c "DROP TABLE documents;"
python ingest.py
```

> Si prefieres evitar el *probe* de dimensiÃ³n en LM Studio, define `EMBEDDING_DIM` en `.env` (p. ej., 768).

---

## 8) Comandos de verificaciÃ³n Ãºtiles

```bash
# Red/puerto abierto
nc -vz 192.168.0.194 1234

# Modelos disponibles
curl -s http://192.168.0.194:1234/v1/models | jq

# Probar embeddings (y ver dimensiÃ³n)
curl -s http://192.168.0.194:1234/v1/embeddings -H "Authorization: Bearer lm-studio" \
  -H "Content-Type: application/json" \
  -d '{"model":"text-embedding-embeddinggemma-300m","input":"hola"}' | jq '.data[0].embedding | length'

# Estado del contenedor Postgres
cd ~/ai-stack/infra && docker compose ps
```

---

## 9) SoluciÃ³n de problemas (FAQ)

- **Timeout/Connection error en embeddings**
  - Comprueba `curl â€¦/v1/models` y `â€¦/v1/embeddings` desde la VM.
  - En LM Studio, usa **Bind `0.0.0.0`** y abre el puerto en el firewall.
  - Verifica que `EMBEDDING_MODEL` exista exactamente en `/v1/models`.

- **`model not found`**
  - El `id` en `.env` no coincide con el listado de `/v1/models`.

- **Error de dimensiÃ³n**
  - Cambiaste de modelo de embeddings â†’ borra/migra `documents` y reingesta.

- **Respuestas vagas / alucinaciones**
  - Baja `temperature` en `llm_client.py` (0.1â€“0.2) y endurece el *system prompt*.
  - Ajusta `k` y `CHUNK_SIZE`/`CHUNK_OVERLAP` para mejorar el contexto relevante.

- **Lentitud**
  - Asegura Ã­ndice HNSW creado; reduce `k` o el tamaÃ±o de chunk; usa un modelo de embeddings mÃ¡s ligero.

---

## 10) Extensiones recomendadas

- **EvaluaciÃ³n de RAG**: integra RAGAS/TruLens para medir *groundedness*, relevancia y cobertura.
- **Reranking**: aÃ±ade un reranker (crossâ€‘encoder) tras el retriever para mejorar precisiÃ³n.
- **Caching**: cachea embeddings y/o respuestas recurrentes.
- **MCP + agentes**: expÃ³n tu vector DB como **resource** y aÃ±ade **tools** para reindexar/evaluar.

---

## 11) Notas de seguridad

- LM Studio no valida realmente el API key â†’ **no** expongas el puerto fuera de tu LAN.
- Limita acceso por firewall a la IP de la VM y mantÃ©n credenciales de Postgres fuera del cÃ³digo.

---

## 12) Licencia / crÃ©ditos

Proyecto de aprendizaje personal. Usa componentes openâ€‘source y LM Studio como servidor local OpenAIâ€‘compatible.

